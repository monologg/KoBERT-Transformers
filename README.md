# KoBERT-Transformers

`KoBERT` & `DistilKoBERT` on ğŸ¤— Huggingface Transformers ğŸ¤—

KoBERT ëª¨ë¸ì€ [ê³µì‹ ë ˆí¬](https://github.com/SKTBrain/KoBERT)ì˜ ê²ƒê³¼ ë™ì¼í•©ë‹ˆë‹¤. ë³¸ ë ˆí¬ëŠ” **Huggingface tokenizerì˜ ëª¨ë“  APIë¥¼ ì§€ì›**í•˜ê¸° ìœ„í•´ì„œ ì œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸš¨ ì¤‘ìš”! ğŸš¨

### ğŸ™ TL;DR

1. `transformers` ëŠ” `v3.0` ì´ìƒì„ ë°˜ë“œì‹œ ì„¤ì¹˜!
2. `tokenizer`ëŠ” ë³¸ ë ˆí¬ì˜ `kobert_transformers/tokenization_kobert.py`ë¥¼ ì‚¬ìš©!

### 1. Tokenizer í˜¸í™˜

`Huggingface Transformers`ê°€ `v2.9.0`ë¶€í„° tokenization ê´€ë ¨ APIê°€ ì¼ë¶€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì— ë§ì¶° ê¸°ì¡´ì˜ `tokenization_kobert.py`ë¥¼ ìƒìœ„ ë²„ì „ì— ë§ê²Œ ìˆ˜ì •í•˜ì˜€ìŠµë‹ˆë‹¤.

### 2. Embeddingì˜ padding_idx ì´ìŠˆ

ì´ì „ë¶€í„° `BertModel`ì˜ `BertEmbeddings`ì—ì„œ `padding_idx=0`ìœ¼ë¡œ **Hard-coding**ë˜ì–´ ìˆì—ˆìŠµë‹ˆë‹¤. (ì•„ë˜ ì½”ë“œ ì°¸ê³ )

```python
class BertEmbeddings(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)
        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)
```

ê·¸ëŸ¬ë‚˜ Sentencepieceì˜ ê²½ìš° ê¸°ë³¸ê°’ìœ¼ë¡œ `pad_token_id=1`, `unk_token_id=0`ìœ¼ë¡œ ì„¤ì •ì´ ë˜ì–´ ìˆê³  (ì´ëŠ” KoBERTë„ ë™ì¼), ì´ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ëŠ” BertModelì˜ ê²½ìš° ì›ì¹˜ ì•Šì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Huggingfaceì—ì„œë„ ìµœê·¼ì— í•´ë‹¹ ì´ìŠˆë¥¼ ì¸ì§€í•˜ì—¬ ì´ë¥¼ ìˆ˜ì •í•˜ì—¬ `v2.9.0`ì— ë°˜ì˜í•˜ì˜€ìŠµë‹ˆë‹¤. ([ê´€ë ¨ PR #3793](https://github.com/huggingface/transformers/pull/3793)) configì— `pad_token_id=1` ì„ ì¶”ê°€ ê°€ëŠ¥í•˜ì—¬ ì´ë¥¼ í•´ê²°í•  ìˆ˜ ìˆê²Œ í•˜ì˜€ìŠµë‹ˆë‹¤.

```python
class BertEmbeddings(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)
        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)
```

ê·¸ëŸ¬ë‚˜ `v.2.9.0`ì—ì„œ `DistilBERT`, `ALBERT` ë“±ì—ëŠ” ì´ ì´ìŠˆê°€ í•´ê²°ë˜ì§€ ì•Šì•„ ì§ì ‘ PRì„ ì˜¬ë ¤ ì²˜ë¦¬í•˜ì˜€ê³  ([ê´€ë ¨ PR #3965](https://github.com/huggingface/transformers/pull/3965)), **`v2.9.1`ì— ìµœì¢…ì ìœ¼ë¡œ ë°˜ì˜ë˜ì–´ ë°°í¬ë˜ì—ˆìŠµë‹ˆë‹¤.**

ì•„ë˜ëŠ” ì´ì „ê³¼ í˜„ì¬ ë²„ì „ì˜ ì°¨ì´ì ì„ ë³´ì—¬ì£¼ëŠ” ì½”ë“œì…ë‹ˆë‹¤.

```python
# Transformers v2.7.0
>>> from transformers import BertModel, DistilBertModel
>>> model = BertModel.from_pretrained("monologg/kobert")
>>> model.embeddings.word_embeddings
Embedding(8002, 768, padding_idx=0)
>>> model = DistilBertModel.from_pretrained("monologg/distilkobert")
>>> model.embeddings.word_embeddings
Embedding(8002, 768, padding_idx=0)


### Transformers v2.9.1
>>> from transformers import BertModel, DistilBertModel
>>> model = BertModel.from_pretrained("monologg/kobert")
>>> model.embeddings.word_embeddings
Embedding(8002, 768, padding_idx=1)
>>> model = DistilBertModel.from_pretrained("monologg/distilkobert")
>>> model.embeddings.word_embeddings
Embedding(8002, 768, padding_idx=1)
```

## KoBERT / DistilKoBERT on ğŸ¤— Transformers ğŸ¤—

### Dependencies

- torch>=1.1.0
- transformers>=3,<5

### How to Use

```python
>>> from transformers import BertModel, DistilBertModel
>>> bert_model = BertModel.from_pretrained('monologg/kobert')
>>> distilbert_model = DistilBertModel.from_pretrained('monologg/distilkobert')
```

**Tokenizerë¥¼ ì‚¬ìš©í•˜ë ¤ë©´, [`kobert_transformers/tokenization_kobert.py`](https://github.com/monologg/KoBERT-Transformers/blob/master/kobert_transformers/tokenization_kobert.py) íŒŒì¼ì„ ë³µì‚¬í•œ í›„, `KoBertTokenizer`ë¥¼ ì„í¬íŠ¸í•˜ë©´ ë©ë‹ˆë‹¤.**

- KoBERTì™€ DistilKoBERT ëª¨ë‘ ë™ì¼í•œ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
- **ê¸°ì¡´ KoBERTì˜ ê²½ìš° Special Tokenì´ ì œëŒ€ë¡œ ë¶„ë¦¬ë˜ì§€ ì•ŠëŠ” ì´ìŠˆ**ê°€ ìˆì–´ì„œ í•´ë‹¹ ë¶€ë¶„ì„ ìˆ˜ì •í•˜ì—¬ ë°˜ì˜í•˜ì˜€ìŠµë‹ˆë‹¤. ([Issue link](https://github.com/SKTBrain/KoBERT/issues/11))

```python
>>> from tokenization_kobert import KoBertTokenizer
>>> tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert') # monologg/distilkobertë„ ë™ì¼
>>> tokenizer.tokenize("[CLS] í•œêµ­ì–´ ëª¨ë¸ì„ ê³µìœ í•©ë‹ˆë‹¤. [SEP]")
>>> ['[CLS]', 'â–í•œêµ­', 'ì–´', 'â–ëª¨ë¸', 'ì„', 'â–ê³µìœ ', 'í•©ë‹ˆë‹¤', '.', '[SEP]']
>>> tokenizer.convert_tokens_to_ids(['[CLS]', 'â–í•œêµ­', 'ì–´', 'â–ëª¨ë¸', 'ì„', 'â–ê³µìœ ', 'í•©ë‹ˆë‹¤', '.', '[SEP]'])
>>> [2, 4958, 6855, 2046, 7088, 1050, 7843, 54, 3]
```

## Kobert-Transformers (Pip library)

[![PyPI](https://img.shields.io/pypi/v/kobert-transformers)](https://pypi.org/project/kobert-transformers/)
[![license](https://img.shields.io/badge/license-Apache%202.0-red)](https://github.com/monologg/DistilKoBERT/blob/master/LICENSE)
[![Downloads](https://pepy.tech/badge/kobert-transformers)](https://pepy.tech/project/kobert-transformers)

- `tokenization_kobert.py`ë¥¼ ë©í•‘í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
- KoBERT, DistilKoBERTë¥¼ Huggingface Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜•íƒœë¡œ ì œê³µ
- `v0.5.0`ì—ì„œëŠ” `transformers v3.0` ì´ìƒìœ¼ë¡œ ê¸°ë³¸ ì„¤ì¹˜í•©ë‹ˆë‹¤. (`transformers v4.0` ê¹Œì§€ëŠ” ì´ìŠˆ ì—†ì´ ì‚¬ìš© ê°€ëŠ¥)

### Install Kobert-Transformers

```bash
pip3 install kobert-transformers
```

### How to Use

```python
>>> import torch
>>> from kobert_transformers import get_kobert_model, get_distilkobert_model
>>> model = get_kobert_model()
>>> model.eval()
>>> input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])
>>> attention_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])
>>> token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])
>>> sequence_output, pooled_output = model(input_ids, attention_mask, token_type_ids)
>>> sequence_output[0]
tensor([[-0.2461,  0.2428,  0.2590,  ..., -0.4861, -0.0731,  0.0756],
        [-0.2478,  0.2420,  0.2552,  ..., -0.4877, -0.0727,  0.0754],
        [-0.2472,  0.2420,  0.2561,  ..., -0.4874, -0.0733,  0.0765]],
       grad_fn=<SelectBackward>)
```

```python
>>> from kobert_transformers import get_tokenizer
>>> tokenizer = get_tokenizer()
>>> tokenizer.tokenize("[CLS] í•œêµ­ì–´ ëª¨ë¸ì„ ê³µìœ í•©ë‹ˆë‹¤. [SEP]")
['[CLS]', 'â–í•œêµ­', 'ì–´', 'â–ëª¨ë¸', 'ì„', 'â–ê³µìœ ', 'í•©ë‹ˆë‹¤', '.', '[SEP]']
>>> tokenizer.convert_tokens_to_ids(['[CLS]', 'â–í•œêµ­', 'ì–´', 'â–ëª¨ë¸', 'ì„', 'â–ê³µìœ ', 'í•©ë‹ˆë‹¤', '.', '[SEP]'])
[2, 4958, 6855, 2046, 7088, 1050, 7843, 54, 3]
```

## Reference

- [KoBERT](https://github.com/SKTBrain/KoBERT)
- [DistilKoBERT](https://github.com/monologg/DistilKoBERT)
- [Huggingface Transformers](https://github.com/huggingface/transformers)
